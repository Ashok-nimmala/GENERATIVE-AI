{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOA3LfyXquMIj8zIdL2yFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashok-nimmala/GENERATIVE-AI/blob/main/ASS_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: (1 ponto) Write Python code without using any libraries to find the value of x at which the function f(x) shown in equation (1) has minimum value. Consider Gradient Descent Algorithm. f(x) = 5x 4 + 3x 2 + 10"
      ],
      "metadata": {
        "id": "x3xEA4USAbfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f_prime(x):\n",
        "    return 20 * x**3 + 6 * x\n",
        "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
        "    x = initial_x\n",
        "    for _ in range(num_iterations):\n",
        "      grad = f_prime(x)\n",
        "      x = x - learning_rate * grad\n",
        "    return x\n",
        "initial_x = 0.5\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "min_x = gradient_descent(initial_x, learning_rate, num_iterations)\n",
        "print(\"The value of x at which f(x) has a minimum is:\", min_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBwVgCTSAkIK",
        "outputId": "01525999-60c8-4141-8c31-d94d3f112f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x at which f(x) has a minimum is: 4.810419162406747e-28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: (1 ponto) Write Python code without using any libraries to find the value of x and y at which the function g(x,y) shown in equation (2) has minimum value. Consider Gradient Descent Algorithm. f(x) = 3x 2 + 5e −y + 10"
      ],
      "metadata": {
        "id": "nQWNK64fBObc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partial_derivative_x(x, y):\n",
        "    return 6 * x\n",
        "def partial_derivative_y(x, y):\n",
        "    return -5 * (2.71828 ** -y)\n",
        "def gradient_descent(initial_x, initial_y, learning_rate, num_iterations):\n",
        "    x = initial_x\n",
        "    y = initial_y\n",
        "    for _ in range(num_iterations):\n",
        "        grad_x = partial_derivative_x(x, y)\n",
        "        grad_y = partial_derivative_y(x, y)\n",
        "        x = x - learning_rate * grad_x\n",
        "        y = y - learning_rate * grad_y\n",
        "    return x, y\n",
        "initial_x = 0.5\n",
        "initial_y = 0.5\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "min_x, min_y = gradient_descent(initial_x, initial_y, learning_rate, num_iterations)\n",
        "print(\"The values of x and y at which g(x, y) has a minimum are:\", min_x, min_y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meb7Lr8iBN6L",
        "outputId": "e4b11fc7-d5b1-48e4-ee4f-fc8be402e5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The values of x and y at which g(x, y) has a minimum are: 6.711561962466847e-28 3.946138890954553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RH9XjAS5Aavr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: (1 ponto) Write Python code without using any libraries to find the value of x at which the sigmoid function z(x) shown in equation (3) has minimum value. Consider Gradient Descent Algorithm. z(x) = 1 1 + e −x"
      ],
      "metadata": {
        "id": "0w_F04veDlZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + 2.71828 ** -x)\n",
        "def sigmoid_derivative(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
        "    x = initial_x\n",
        "    for _ in range(num_iterations):\n",
        "        grad = sigmoid_derivative(x)\n",
        "        x = x - learning_rate * grad\n",
        "    return x\n",
        "initial_x = 0.5\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "min_x = gradient_descent(initial_x, learning_rate, num_iterations)\n",
        "print(\"The value of x at which z(x) has a minimum is:\", min_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-v2bCtiDoMu",
        "outputId": "f5b7be3d-0d1f-48bb-b914-3b72cf265f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x at which z(x) has a minimum is: -1.6012961162961212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: (1 ponto) Write Python code without using any libraries to find the value of optimal values of model parameters M and C such that the model’s Square Error Value shown in equation 4 will be minimum. It means model gives output close to expected output as shown in SE = (ExpectedOutput − P redictedOutput) 2"
      ],
      "metadata": {
        "id": "00592mAND_Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(x, y_true, M, C):\n",
        "    y_pred = M * x + C\n",
        "    error = y_true - y_pred\n",
        "    gradient_M = -2 * x * error\n",
        "    gradient_C = -2 * error\n",
        "    return gradient_M, gradient_C\n",
        "def gradient_descent(x, y_true, initial_M, initial_C, learning_rate, num_iterations):\n",
        "    M = initial_M\n",
        "    C = initial_C\n",
        "    for _ in range(num_iterations):\n",
        "        total_gradient_M = 0\n",
        "        total_gradient_C = 0\n",
        "        for i in range(len(x)):\n",
        "            grad_M, grad_C = compute_gradients(x[i], y_true[i], M, C)\n",
        "            total_gradient_M += grad_M\n",
        "            total_gradient_C += grad_C\n",
        "        M = M - learning_rate * (total_gradient_M / len(x))\n",
        "        C = C - learning_rate * (total_gradient_C / len(x))\n",
        "    return M, C\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y_true = [3, 7, 11, 15, 19]\n",
        "initial_M = 0.0\n",
        "initial_C = 0.0\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "optimal_M, optimal_C = gradient_descent(x, y_true, initial_M, initial_C, learning_rate, num_iterations)\n",
        "print(\"The optimal values of M and C are:\", optimal_M, optimal_C)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXh4nogJEAVv",
        "outputId": "2ba755bf-dc5e-4b22-8568-66bcb8b315b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal values of M and C are: 3.981660469673651 -0.9337884764204182\n"
          ]
        }
      ]
    }
  ]
}